Dataset,Model,Feature,Accuracy,Precision,Recall,F1
thehamkercat_telegram-spam-ham,mshenoda/roberta-spam,learning_rate_1e-05,0.9972085708669155,0.9972254050318522,0.9933125318968274,0.9952638655041021
thehamkercat_telegram-spam-ham,mshenoda/roberta-spam,learning_rate_3e-05,0.99740515038333,0.9967341974773152,0.9944770802266791,0.9956021991077757
thehamkercat_telegram-spam-ham,mshenoda/roberta-spam,weight_decay_0.1,0.9977295065854138,0.9975959567040638,0.9947099699657811,0.9961502734566292
thehamkercat_telegram-spam-ham,mshenoda/roberta-spam,weight_decay_0.001,0.9977000196579517,0.9973296130924322,0.9948762818234856,0.996100887338763
thehamkercat_telegram-spam-ham,prithivMLmods/Spam-Bert-Uncased,learning_rate_1e-05,0.9799685472773737,0.9744547629038834,0.9569140587377657,0.9654862700687126
thehamkercat_telegram-spam-ham,prithivMLmods/Spam-Bert-Uncased,learning_rate_3e-05,0.9885590721446824,0.9871670164967957,0.9737166266468726,0.9802652125504542
thehamkercat_telegram-spam-ham,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.1,0.9867013957145666,0.9856802829299374,0.968792221436218,0.9770136130398039
thehamkercat_telegram-spam-ham,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.001,0.9867112246903872,0.9858468033366241,0.9686591320963174,0.9770305320710662
FredZhang7_all-scam-spam,mshenoda/roberta-spam,learning_rate_1e-05,0.9945470706677784,0.9957751439345748,0.9908558972005508,0.993300783792975
FredZhang7_all-scam-spam,mshenoda/roberta-spam,learning_rate_3e-05,0.9964335455159044,0.9971485623683604,0.994114272602111,0.9956251494274249
FredZhang7_all-scam-spam,mshenoda/roberta-spam,weight_decay_0.1,0.9960628226983183,0.9967307399013319,0.9936209270307481,0.9951649747561848
FredZhang7_all-scam-spam,mshenoda/roberta-spam,weight_decay_0.001,0.995954890167315,0.9968785102610067,0.9932078935291418,0.995032063685936
FredZhang7_all-scam-spam,prithivMLmods/Spam-Bert-Uncased,learning_rate_1e-05,0.9887891158143434,0.9910847253422637,0.9813217072051401,0.9861396625518897
FredZhang7_all-scam-spam,prithivMLmods/Spam-Bert-Uncased,learning_rate_3e-05,0.9941341284712824,0.9951516518807232,0.9904428636989444,0.9927735813922324
FredZhang7_all-scam-spam,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.1,0.9928764842248026,0.9943086758219899,0.9881826525929325,0.9912024376219716
FredZhang7_all-scam-spam,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.001,0.9927591648871374,0.9941821973985079,0.9880220284534189,0.9910586603493854
prithivMLmods_Spam-Text-Detect-Analysis,mshenoda/roberta-spam,learning_rate_1e-05,0.9998564249820532,0.9989326203208556,1.0,0.9994652401638234
prithivMLmods_Spam-Text-Detect-Analysis,mshenoda/roberta-spam,learning_rate_3e-05,0.9997487437185931,0.9997333333333334,0.9983950050895327,0.9990618382827222
prithivMLmods_Spam-Text-Detect-Analysis,mshenoda/roberta-spam,weight_decay_0.1,0.9996410624551327,0.9986807387862797,0.9986623847686772,0.99866556815842
prithivMLmods_Spam-Text-Detect-Analysis,mshenoda/roberta-spam,weight_decay_0.001,0.9996410624551327,0.9986807387862797,0.9986623847686772,0.99866556815842
prithivMLmods_Spam-Text-Detect-Analysis,prithivMLmods/Spam-Bert-Uncased,learning_rate_1e-05,0.9993539124192392,0.999192079780315,0.995983570128027,0.9975817174756051
prithivMLmods_Spam-Text-Detect-Analysis,prithivMLmods/Spam-Bert-Uncased,learning_rate_3e-05,0.9989949748743718,0.9975899255740615,0.9949126177402474,0.9962401287794412
prithivMLmods_Spam-Text-Detect-Analysis,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.1,0.9991385498923189,0.9983913515990167,0.9951799974193918,0.9967774212450011
prithivMLmods_Spam-Text-Detect-Analysis,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.001,0.9991385498923189,0.9983913515990167,0.9951799974193918,0.9967774212450011
