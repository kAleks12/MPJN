Dataset,Model,Feature,Accuracy,Precision,Recall,F1
thehamkercat_telegram-spam-ham,mshenoda/roberta-spam,learning_rate_1e-05,0.995847257715746,0.9958150825397518,0.9901018816499003,0.9929499953238639
thehamkercat_telegram-spam-ham,mshenoda/roberta-spam,learning_rate_3e-05,0.9958226852761941,0.9945004590543929,0.9913495250209509,0.9929208016216052
thehamkercat_telegram-spam-ham,mshenoda/roberta-spam,weight_decay_0.1,0.9962404167485748,0.9954910642073754,0.9917654707224487,0.9936247146795197
thehamkercat_telegram-spam-ham,mshenoda/roberta-spam,weight_decay_0.001,0.9961666994299194,0.9954073964821265,0.9915990813713671,0.9934995270926812
thehamkercat_telegram-spam-ham,prithivMLmods/Spam-Bert-Uncased,learning_rate_1e-05,0.9653528602319639,0.9587554371088864,0.9219796956281558,0.9398882071666409
thehamkercat_telegram-spam-ham,prithivMLmods/Spam-Bert-Uncased,learning_rate_3e-05,0.9782042461175545,0.9781750750212075,0.9471005022677883,0.962200595874739
thehamkercat_telegram-spam-ham,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.1,0.9747886770198545,0.9757231036355176,0.937700915418193,0.9561615891955931
thehamkercat_telegram-spam-ham,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.001,0.9748132494594063,0.9757241033245286,0.9377841100937337,0.9562043298476783
FredZhang7_all-scam-spam,mshenoda/roberta-spam,learning_rate_1e-05,0.9909430710491085,0.991720411575375,0.9861042387568817,0.988892689989724
FredZhang7_all-scam-spam,mshenoda/roberta-spam,learning_rate_3e-05,0.9938877546910414,0.9942116376699508,0.990831720504608,0.9925115317214785
FredZhang7_all-scam-spam,mshenoda/roberta-spam,weight_decay_0.1,0.9928318947460286,0.9940247828463717,0.9884250505849199,0.991204722547862
FredZhang7_all-scam-spam,mshenoda/roberta-spam,weight_decay_0.001,0.9929374828326042,0.9940545223624451,0.9886542638112756,0.9913342713138575
FredZhang7_all-scam-spam,prithivMLmods/Spam-Bert-Uncased,learning_rate_1e-05,0.9784135048103062,0.9815930289697128,0.9653319263906502,0.9733818066682113
FredZhang7_all-scam-spam,prithivMLmods/Spam-Bert-Uncased,learning_rate_3e-05,0.988068810478975,0.9896644775787129,0.981061682407481,0.985330026924576
FredZhang7_all-scam-spam,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.1,0.9859101604094599,0.9869808630408342,0.9784543803158496,0.9826908673450134
FredZhang7_all-scam-spam,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.001,0.9859453557042572,0.9870081048300963,0.9785116934734452,0.9827328157967472
prithivMLmods_Spam-Text-Detect-Analysis,mshenoda/roberta-spam,learning_rate_1e-05,0.9996410624551328,0.9973315508021391,1.0,0.9986631004095585
prithivMLmods_Spam-Text-Detect-Analysis,mshenoda/roberta-spam,learning_rate_3e-05,0.9998205312275665,1.0,0.9986613095152757,0.9993297575087823
prithivMLmods_Spam-Text-Detect-Analysis,mshenoda/roberta-spam,weight_decay_0.1,0.9996410624551328,1.0,0.9973262032085561,0.9986595174262733
prithivMLmods_Spam-Text-Detect-Analysis,mshenoda/roberta-spam,weight_decay_0.001,0.9996410624551328,1.0,0.9973262032085561,0.9986595174262733
prithivMLmods_Spam-Text-Detect-Analysis,prithivMLmods/Spam-Bert-Uncased,learning_rate_1e-05,0.9988334529791816,0.9986486486486487,0.992632722111511,0.9956303491412466
prithivMLmods_Spam-Text-Detect-Analysis,prithivMLmods/Spam-Bert-Uncased,learning_rate_3e-05,0.998653984206748,0.9966468281954031,0.9933011713093719,0.9949645942389764
prithivMLmods_Spam-Text-Detect-Analysis,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.1,0.9984745154343145,0.9966468281954031,0.99196427291365,0.9942907495043344
prithivMLmods_Spam-Text-Detect-Analysis,prithivMLmods/Spam-Bert-Uncased,weight_decay_0.001,0.9984745154343145,0.9966468281954031,0.99196427291365,0.9942907495043344
